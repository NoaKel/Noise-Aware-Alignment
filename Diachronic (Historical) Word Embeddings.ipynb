{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to downloaded historical embeddings. example:\n",
    "path_to_historical_emb = 'C:\\\\Users\\\\user\\\\Downloads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from noise_aware import noise_aware\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings and index them\n",
    "def load_historical_emb(year):\n",
    "    f = open(path_to_historical_emb + '\\\\eng-fiction-all\\\\sgns\\\\' + str(year) + '-vocab.pkl', 'rb')\n",
    "    iw = pickle.load(f)\n",
    "    emb_array = np.load(path_to_historical_emb + '\\\\eng-fiction-all\\\\sgns\\\\' + str(year) + '-w.npy')\n",
    "    word2idx = {word: i for i, word in enumerate(iw)}\n",
    "    idx2word = {i: word for i, word in enumerate(iw)}\n",
    "    f = open(path_to_historical_emb + '\\\\eng-fiction-all\\\\pos\\\\' + str(year) + '-pos.pkl', 'rb')\n",
    "    pos = pickle.load(f)\n",
    "    return emb_array, np.array(iw), word2idx, idx2word, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top changed words\n",
    "def get_most_changed_words(A, B, idx2word, Q, num_of_words=10, f_indices=None):\n",
    "    threshold = 10^-5\n",
    "    # load dict of full non-stop non-proper nouns words\n",
    "    f = open(path_to_historical_emb + '\\\\eng-fiction-all\\\\word_lists\\\\full-nstop_nproper.pkl', 'rb')\n",
    "    full_nstop_nproper = pickle.load(f, encoding='latin1')\n",
    "    # load frequencies\n",
    "    f = open(path_to_historical_emb + '\\\\eng-fiction-all\\\\freqs.pkl', 'rb')\n",
    "    freqs = pickle.load(f, encoding='latin1')\n",
    "\n",
    "    n, dim = A.shape\n",
    "    distances_vector_indexed = []\n",
    "    for i in range (n):\n",
    "        sim = 1 - cosine(np.dot(A[i, :],Q),B[i, :])\n",
    "        distances_vector_indexed.append((sim, idx2word[i]))  \n",
    "        \n",
    "    distances_vector_sorted = sorted(distances_vector_indexed)\n",
    "    # print top changed words\n",
    "    i = 0\n",
    "    while (i < num_of_words):\n",
    "        word = distances_vector_sorted[i][1]\n",
    "        if freqs[word][1900] > threshold and freqs[word][1990] > threshold and word in full_nstop_nproper:\n",
    "            print (i, 'sim:', round(distances_vector_sorted[i][0],3), 'word:', distances_vector_sorted[i][1])\n",
    "            i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean zerored embeddings\n",
    "def clean_zeros(nonzero_idxs, array, words):\n",
    "    array = array[list(nonzero_idxs), :]\n",
    "    words = words[list(nonzero_idxs)]\n",
    "    word2idx = {word: i for i, word in enumerate(words)}\n",
    "    idx2word = {i: word for i, word in enumerate(words)}\n",
    "    return array, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load historical embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load historical embeddings\n",
    "mat1900, words1900, word2idx1900, idx2word1900, pos1900 = load_historical_emb(1900)\n",
    "mat1990, words1990, word2idx1990, idx2word1990, pos1990 = load_historical_emb(1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder matrices\n",
    "idx_list = [word2idx1990[idx2word1900[i]] for i in range (0,len(word2idx1900))]\n",
    "array_1900 = mat1900\n",
    "array_1990 = mat1990[idx_list, :]\n",
    "words1990 = words1990[idx_list]\n",
    "\n",
    "# clean zero embeddings\n",
    "f = open(path_to_historical_emb + '\\\\eng-fiction-all\\\\word_lists\\\\full-nstop_nproper.pkl', 'rb')\n",
    "full_nstop_nproper = pickle.load(f, encoding='latin1')\n",
    "\n",
    "rows1900, _ = np.nonzero(array_1900)\n",
    "idxs_1900 = set(rows1900)\n",
    "rows1990, _ = np.nonzero(array_1990)\n",
    "idxs_1990 = set(rows1990)\n",
    "nonzero_idxs = idxs_1900.intersection(idxs_1990)\n",
    "nstop_nproper_idx = {i for i, word in enumerate(words1900) if word in full_nstop_nproper}\n",
    "clean_idxs = nonzero_idxs.intersection(nstop_nproper_idx)\n",
    "\n",
    "array_1900, word2idx_ordered, idx2word_ordered = clean_zeros(clean_idxs, array_1900, words1900)\n",
    "array_1990, _, _ = clean_zeros(clean_idxs, array_1990, words1990)\n",
    "\n",
    "n, dim = array_1900.shape\n",
    "init_Q, _ = orthogonal_procrustes(array_1900, array_1990)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise Aware Aligment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 alpha: 0.57 sigma: 0.003 sigmay 0.003\n",
      "iter: 1 alpha: 0.553 sigma: 0.003 sigmay 0.003\n",
      "iter: 2 alpha: 0.548 sigma: 0.003 sigmay 0.003\n",
      "iter: 3 alpha: 0.547 sigma: 0.003 sigmay 0.003\n",
      "iter: 4 alpha: 0.546 sigma: 0.003 sigmay 0.003\n",
      "iter: 5 alpha: 0.546 sigma: 0.003 sigmay 0.003\n",
      "iter: 6 alpha: 0.545 sigma: 0.003 sigmay 0.003\n",
      "iter: 7 alpha: 0.545 sigma: 0.003 sigmay 0.003\n",
      "iter: 8 alpha: 0.545 sigma: 0.003 sigmay 0.003\n",
      "iter: 9 alpha: 0.545 sigma: 0.003 sigmay 0.003\n"
     ]
    }
   ],
   "source": [
    "Q_pred, alpha_pred, t_indices_pred, f_indices_pred = \\\n",
    "noise_aware(array_1900, array_1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sim: -0.003 word: guy\n",
      "1 sim: 0.073 word: 31\n",
      "2 sim: 0.084 word: ignored\n",
      "3 sim: 0.087 word: overdue\n",
      "4 sim: 0.088 word: 2\n",
      "5 sim: 0.093 word: vis\n",
      "6 sim: 0.108 word: ad\n",
      "7 sim: 0.115 word: notices\n",
      "8 sim: 0.121 word: random\n",
      "9 sim: 0.123 word: 27\n"
     ]
    }
   ],
   "source": [
    "get_most_changed_words(array_1900, array_1990, idx2word_ordered, num_of_words=10, Q=Q_pred, f_indices=f_indices_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: wanting is_clean: False sim 0.19206961647592302\n",
      "word: gay is_clean: False sim 0.2759707895493978\n",
      "word: check is_clean: False sim 0.25367264136509693\n",
      "word: starting is_clean: False sim 0.2740132369190764\n",
      "word: major is_clean: False sim 0.26918349630846394\n",
      "word: actually is_clean: False sim 0.24110088125204576\n",
      "word: touching is_clean: False sim 0.4542916169674803\n",
      "word: harry is_clean: False sim 0.2627476280889731\n",
      "word: headed is_clean: False sim 0.2993917518480882\n",
      "word: romance is_clean: False sim 0.2974979837761651\n"
     ]
    }
   ],
   "source": [
    "words = ['wanting', 'gay', 'check', 'starting', 'major', 'actually', 'touching', 'harry', 'headed', 'romance']\n",
    "\n",
    "for word in words:\n",
    "    sim =  1 - cosine(np.dot(array_1900[word2idx_ordered[word], :],Q_pred),array_1990[word2idx_ordered[word], :])\n",
    "    print ('word:', word, 'is_clean:', word2idx_ordered[word] in t_indices_pred, 'sim', sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32600"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unchanged_historical = [idx2word_ordered[idx] for idx in t_indices_pred]\n",
    "f = open(\"unchanged_historical.txt\", \"w\", encoding=\"utf-8\")\n",
    "f.write('\\n'.join(unchanged_historical))\n",
    "changed_historical = [idx2word_ordered[idx] for idx in f_indices_pred]\n",
    "f = open(\"changed_historical.txt\", \"w\", encoding=\"utf-8\")\n",
    "f.write('\\n'.join(changed_historical))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
