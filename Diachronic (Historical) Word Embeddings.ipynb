{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to downloaded historical embeddings. example:\n",
    "path_to_historical_emb = 'C:\\\\Users\\\\user\\\\Downloads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from noise_aware import noise_aware\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings and index them\n",
    "def load_historical_emb(year):\n",
    "    f = open(path_to_historical_emb + '\\\\eng-fiction-all\\\\sgns\\\\' + str(year) + '-vocab.pkl', 'rb')\n",
    "    iw = pickle.load(f)\n",
    "    emb_array = np.load(path_to_historical_emb + '\\\\eng-fiction-all\\\\sgns\\\\' + str(year) + '-w.npy')\n",
    "    word2idx = {word: i for i, word in enumerate(iw)}\n",
    "    idx2word = {i: word for i, word in enumerate(iw)}\n",
    "    f = open(path_to_historical_emb + '\\\\eng-fiction-all\\\\pos\\\\' + str(year) + '-pos.pkl', 'rb')\n",
    "    pos = pickle.load(f)\n",
    "    return emb_array, np.array(iw), word2idx, idx2word, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top changed words\n",
    "def get_most_changed_words(A, B, idx2word, Q, num_of_words=10, f_indices=None):\n",
    "    threshold = 10^-5\n",
    "    # load dict of full non-stop non-proper nouns words\n",
    "    f = open(path_to_historical_emb + '\\\\eng-fiction-all\\\\word_lists\\\\full-nstop_nproper.pkl', 'rb')\n",
    "    full_nstop_nproper = pickle.load(f, encoding='latin1')\n",
    "    # load frequencies\n",
    "    f = open(path_to_historical_emb + '\\\\eng-fiction-all\\\\freqs.pkl', 'rb')\n",
    "    freqs = pickle.load(f, encoding='latin1')\n",
    "\n",
    "    n, dim = A.shape\n",
    "    distances_vector_indexed = []\n",
    "    for i in range (n):\n",
    "        sim = 1 - cosine(np.dot(A[i, :],Q),B[i, :])\n",
    "        distances_vector_indexed.append((sim, idx2word[i]))  \n",
    "        \n",
    "    distances_vector_sorted = sorted(distances_vector_indexed)\n",
    "    # print top changed words\n",
    "    i = 0\n",
    "    while (i < num_of_words):\n",
    "        word = distances_vector_sorted[i][1]\n",
    "        if freqs[word][1900] > threshold and freqs[word][1990] > threshold and word in full_nstop_nproper:\n",
    "            print (i, 'sim:', round(distances_vector_sorted[i][0],3), 'word:', distances_vector_sorted[i][1])\n",
    "            i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean zerored embeddings\n",
    "def clean_zeros(nonzero_idxs, array, words):\n",
    "    array = array[list(nonzero_idxs), :]\n",
    "    words = words[list(nonzero_idxs)]\n",
    "    word2idx = {word: i for i, word in enumerate(words)}\n",
    "    idx2word = {i: word for i, word in enumerate(words)}\n",
    "    return array, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load historical embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load historical embeddings\n",
    "mat1900, words1900, word2idx1900, idx2word1900, pos1900 = load_historical_emb(1900)\n",
    "mat1990, words1990, word2idx1990, idx2word1990, pos1990 = load_historical_emb(1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder matrices\n",
    "idx_list = [word2idx1990[idx2word1900[i]] for i in range (0,len(word2idx1900))]\n",
    "array_1900 = mat1900\n",
    "array_1990 = mat1990[idx_list, :]\n",
    "words1990 = words1990[idx_list]\n",
    "\n",
    "# clean zero embeddings\n",
    "f = open(path_to_historical_emb + '\\\\eng-fiction-all\\\\word_lists\\\\full-nstop_nproper.pkl', 'rb')\n",
    "full_nstop_nproper = pickle.load(f, encoding='latin1')\n",
    "\n",
    "rows1900, _ = np.nonzero(array_1900)\n",
    "idxs_1900 = set(rows1900)\n",
    "rows1990, _ = np.nonzero(array_1990)\n",
    "idxs_1990 = set(rows1990)\n",
    "nonzero_idxs = idxs_1900.intersection(idxs_1990)\n",
    "nstop_nproper_idx = {i for i, word in enumerate(words1900) if word in full_nstop_nproper}\n",
    "clean_idxs = nonzero_idxs.intersection(nstop_nproper_idx)\n",
    "\n",
    "array_1900, word2idx_ordered, idx2word_ordered = clean_zeros(clean_idxs, array_1900, words1900)\n",
    "array_1990, _, _ = clean_zeros(clean_idxs, array_1990, words1990)\n",
    "\n",
    "n, dim = array_1900.shape\n",
    "init_Q, _ = orthogonal_procrustes(array_1900, array_1990)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise Aware Aligment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 alpha: 0.57 sigma: 0.003 sigmay 0.003\n",
      "iter: 1 alpha: 0.553 sigma: 0.003 sigmay 0.003\n",
      "iter: 2 alpha: 0.548 sigma: 0.003 sigmay 0.003\n",
      "iter: 3 alpha: 0.547 sigma: 0.003 sigmay 0.003\n",
      "iter: 4 alpha: 0.546 sigma: 0.003 sigmay 0.003\n",
      "iter: 5 alpha: 0.546 sigma: 0.003 sigmay 0.003\n",
      "iter: 6 alpha: 0.545 sigma: 0.003 sigmay 0.003\n",
      "iter: 7 alpha: 0.545 sigma: 0.003 sigmay 0.003\n",
      "iter: 8 alpha: 0.545 sigma: 0.003 sigmay 0.003\n",
      "iter: 9 alpha: 0.545 sigma: 0.003 sigmay 0.003\n"
     ]
    }
   ],
   "source": [
    "Q_pred, alpha_pred, t_indices_pred, f_indices_pred = \\\n",
    "noise_aware(array_1900, array_1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'collections\\r'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-535d1b336a23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_most_changed_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray_1900\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray_1990\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx2word_ordered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mQ_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf_indices_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-76-e82d8cd7d0fd>\u001b[0m in \u001b[0;36mget_most_changed_words\u001b[1;34m(A, B, idx2word, Q, num_of_words, f_indices)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# load frequencies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_historical_emb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\eng-fiction-all\\\\freqs2.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mfreqs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'collections\\r'"
     ]
    }
   ],
   "source": [
    "get_most_changed_words(array_1900, array_1990, idx2word_ordered, num_of_words=10, Q=Q_pred, f_indices=f_indices_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: wanting is_clean: False sim 0.19206961647592302\n",
      "word: gay is_clean: False sim 0.2759707895493978\n",
      "word: check is_clean: False sim 0.25367264136509693\n",
      "word: starting is_clean: False sim 0.2740132369190764\n",
      "word: major is_clean: False sim 0.26918349630846394\n",
      "word: actually is_clean: False sim 0.24110088125204576\n",
      "word: touching is_clean: False sim 0.4542916169674803\n",
      "word: harry is_clean: False sim 0.2627476280889731\n",
      "word: headed is_clean: False sim 0.2993917518480882\n",
      "word: romance is_clean: False sim 0.2974979837761651\n"
     ]
    }
   ],
   "source": [
    "words = ['wanting', 'gay', 'check', 'starting', 'major', 'actually', 'touching', 'harry', 'headed', 'romance']\n",
    "\n",
    "for word in words:\n",
    "    sim =  1 - cosine(np.dot(array_1900[word2idx_ordered[word], :],Q_pred),array_1990[word2idx_ordered[word], :])\n",
    "    print ('word:', word, 'is_clean:', word2idx_ordered[word] in t_indices_pred, 'sim', sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32600"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unchanged_historical = [idx2word_ordered[idx] for idx in t_indices_pred]\n",
    "f = open(\"unchanged_historical.txt\", \"w\", encoding=\"utf-8\")\n",
    "f.write('\\n'.join(unchanged_historical))\n",
    "changed_historical = [idx2word_ordered[idx] for idx in f_indices_pred]\n",
    "f = open(\"changed_historical.txt\", \"w\", encoding=\"utf-8\")\n",
    "f.write('\\n'.join(changed_historical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
